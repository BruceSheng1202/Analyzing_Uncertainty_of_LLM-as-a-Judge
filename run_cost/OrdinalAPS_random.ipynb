{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_number = {\n",
    "    'one': 1,\n",
    "    'two': 2,\n",
    "    'three': 3,\n",
    "    'four': 4,\n",
    "    'five': 5\n",
    "}\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "\n",
    "alpha = 0.10\n",
    "\n",
    "import json\n",
    "import math\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import os, sys\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qhat_ordinal_aps(prediction_function, cal_scores, cal_labels, alpha):\n",
    "    n = cal_scores.shape[0]\n",
    "    grid_size = 10000\n",
    "    for q in np.linspace(1e-3, 1 - 1e-3, grid_size)[::-1]:\n",
    "        coverage, _, _ = evaluate_sets(prediction_function, np.copy(cal_scores), np.copy(cal_labels), q, alpha)\n",
    "        if coverage <= (np.ceil((n + 1)*(1 - alpha))/n):\n",
    "            # return q + 1/(grid_size - 1)\n",
    "            return np.minimum(q + 1/(grid_size - 1), 1.0 - 1e-6)  # Clip q to be less than 1.0\n",
    "    return q\n",
    "\n",
    "def evaluate_sets(prediction_function, val_scores, val_labels, qhat, alpha, print_bool=False):\n",
    "    sets = prediction_function(val_scores, qhat)\n",
    "    # Check\n",
    "    sizes = sets.sum(axis=1)\n",
    "    sizes_distribution = np.array([(sizes == i).mean() for i in range(5)])\n",
    "    # Evaluate coverage\n",
    "    covered = sets[np.arange(val_labels.shape[0]), val_labels]\n",
    "    coverage = covered.mean()\n",
    "    label_stratified_coverage = [\n",
    "        covered[val_labels == j].mean() for j in range(np.unique(val_labels).max() + 1)\n",
    "    ]\n",
    "    label_distribution = [\n",
    "        (val_labels == j).mean() for j in range(np.unique(val_labels).max() + 1)\n",
    "    ]\n",
    "    if(print_bool):\n",
    "        print(r'$\\alpha$' + f\":{alpha}  |  coverage: {coverage}  |  average size: {sizes.mean()}  |  qhat: {qhat}  |  set size distribution: {sizes_distribution} \")\n",
    "        print(f\"label stratified coverage: {label_stratified_coverage}  \\nlabel distribution: {label_distribution}\")\n",
    "    return coverage, label_stratified_coverage, sizes_distribution\n",
    "\n",
    "def ordinal_aps_prediction(val_scores, qhat):\n",
    "    import numpy as np\n",
    "\n",
    "    n_samples, n_classes = val_scores.shape\n",
    "    P = val_scores == val_scores.max(axis=1)[:, None]\n",
    "\n",
    "    idx_construction_incomplete = (val_scores * P.astype(float)).sum(axis=1) <= qhat\n",
    "\n",
    "    max_iter = n_classes  \n",
    "    iter_count = 0\n",
    "\n",
    "    while idx_construction_incomplete.sum() > 0:\n",
    "        iter_count += 1\n",
    "        if iter_count > max_iter:\n",
    "            P[idx_construction_incomplete] = True\n",
    "            break\n",
    "\n",
    "        P_inc = P[idx_construction_incomplete]\n",
    "        scores_inc = val_scores[idx_construction_incomplete]\n",
    "\n",
    "        set_cumsum = P_inc.cumsum(axis=1)\n",
    "        lower_edge_idx = (P_inc > 0).argmax(axis=1)\n",
    "        upper_edge_idx = set_cumsum.argmax(axis=1)\n",
    "\n",
    "        left_valid = (lower_edge_idx - 1) >= 0\n",
    "        right_valid = (upper_edge_idx + 1) < scores_inc.shape[1]\n",
    "\n",
    "        lower_edge_wins = np.zeros(scores_inc.shape[0], dtype=bool)\n",
    "\n",
    "        lower_edge_wins[~right_valid & left_valid] = True\n",
    "\n",
    "        both_valid = left_valid & right_valid\n",
    "        lower_scores = scores_inc[np.arange(scores_inc.shape[0])[both_valid], lower_edge_idx[both_valid] - 1]\n",
    "        upper_scores = scores_inc[np.arange(scores_inc.shape[0])[both_valid], upper_edge_idx[both_valid] + 1]\n",
    "        lower_edge_wins[both_valid] = lower_scores > upper_scores\n",
    "\n",
    "        valid_left = lower_edge_wins & ((lower_edge_idx - 1) >= 0)\n",
    "        P_inc[valid_left, lower_edge_idx[valid_left] - 1] = True\n",
    "\n",
    "        valid_right = (~lower_edge_wins) & ((upper_edge_idx + 1) < scores_inc.shape[1])\n",
    "        P_inc[valid_right, upper_edge_idx[valid_right] + 1] = True\n",
    "\n",
    "        P[idx_construction_incomplete] = P_inc\n",
    "\n",
    "        idx_construction_incomplete = (val_scores * P.astype(float)).sum(axis=1) <= qhat\n",
    "\n",
    "    return P\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "    return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "\n",
    "def run_experiment(X, y, seed, dataset='summeval', dimension='consistency'):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    X.columns = list(range(len(X.columns)))\n",
    "    X = X.to_numpy().astype(np.float32)\n",
    "    y = y.to_numpy().astype(np.float32)-1\n",
    "\n",
    "    y = y.astype(int)\n",
    "    x_arr = X\n",
    "    from scipy.interpolate import interp1d\n",
    "    n = x_arr.shape[0]\n",
    "\n",
    "    new_x = softmax(x_arr)\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    fyxs_cal, fyxs_test, y_cal, y_test = train_test_split(new_x, y, test_size=0.5, random_state=seed)\n",
    "    y_cal = y_cal.ravel()\n",
    "    y_test = y_test.ravel()\n",
    "\n",
    "    cal_scores = fyxs_cal\n",
    "    cal_labels = y_cal\n",
    "    test_scores = fyxs_test\n",
    "    test_labels = y_test\n",
    "\n",
    "    qhat = get_qhat_ordinal_aps(ordinal_aps_prediction, np.copy(cal_scores), np.copy(cal_labels), alpha)\n",
    "    test_pred_sets = ordinal_aps_prediction(np.copy(test_scores), qhat)\n",
    "    prediction_intervals = []\n",
    "    for pred_set in test_pred_sets:\n",
    "        indices = np.where(pred_set)[0]\n",
    "        if len(indices) > 0:\n",
    "            interval = (indices.min(), indices.max())\n",
    "        else:\n",
    "            interval = None \n",
    "        prediction_intervals.append(interval)\n",
    "\n",
    "    y_qlow, y_qup = zip(*prediction_intervals)\n",
    "    y_qlow = np.array(y_qlow)+1\n",
    "    y_qup = np.array(y_qup)+1\n",
    "\n",
    "    y_test_real = test_labels/3+1\n",
    "    # y_qlow = np.array(y_qlow)+1\n",
    "    # y_qup = np.array(y_qup)+1\n",
    "\n",
    "    # y_test_real = test_labels+1\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'low':    y_qlow.ravel(),\n",
    "        'up':     y_qup.ravel(),\n",
    "        'y_test': y_test_real.ravel(),\n",
    "    })\n",
    "\n",
    "    df.to_csv(f'OrdinalAPS_{dataset}_{dimension}_{seed}.csv', index=False)\n",
    "\n",
    "    in_interval = (y_test_real >= y_qlow) & (y_test_real <= y_qup)\n",
    "\n",
    "    average_width = np.mean(y_qup-y_qlow)\n",
    "    coverage_rate = np.mean(in_interval)\n",
    "\n",
    "    print(f\"Seed: {seed}, Width: {average_width:.4f}, Coverage: {coverage_rate:.4f}\")\n",
    "\n",
    "    return average_width, coverage_rate\n",
    "\n",
    "\n",
    "import time\n",
    "import tracemalloc\n",
    "def calculate_statistics(X, y, num_runs=100, seed_start=1, dataset='Summeval', dimension='consistency'):\n",
    "    from tqdm import tqdm\n",
    "    timecost = []\n",
    "    memory = []\n",
    "    for i in tqdm(range(num_runs), desc=\"Running experiments\"):\n",
    "        seed = seed_start + i\n",
    "        tracemalloc.start()\n",
    "        start = time.perf_counter()\n",
    "        average_width, coverage_rate = run_experiment(X, y, seed, dataset, dimension)\n",
    "        end = time.perf_counter()\n",
    "        current, peak = tracemalloc.get_traced_memory()\n",
    "        tracemalloc.stop()\n",
    "        timecost.append(end - start)\n",
    "        memory.append(peak)\n",
    "\n",
    "    mean_time = np.mean(timecost)\n",
    "    std_time = np.std(timecost)\n",
    "    mean_memory = np.mean(memory)\n",
    "    std_memory = np.std(memory)\n",
    "\n",
    "    return  mean_time, std_time, mean_memory, std_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running experiments: 100%|██████████| 10/10 [00:01<00:00,  7.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 1, Width: 0.7449, Coverage: 0.4184\n",
      "Seed: 2, Width: 0.7449, Coverage: 0.3571\n",
      "Seed: 3, Width: 0.7143, Coverage: 0.3980\n",
      "Seed: 4, Width: 0.7041, Coverage: 0.3571\n",
      "Seed: 5, Width: 0.7143, Coverage: 0.3776\n",
      "Seed: 6, Width: 0.7143, Coverage: 0.3469\n",
      "Seed: 7, Width: 0.7347, Coverage: 0.3673\n",
      "Seed: 8, Width: 0.6531, Coverage: 0.2959\n",
      "Seed: 9, Width: 0.7245, Coverage: 0.3367\n",
      "Seed: 10, Width: 0.6837, Coverage: 0.2857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running experiments: 100%|██████████| 10/10 [00:00<00:00, 130.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 1, Width: 0.2857, Coverage: 0.1238\n",
      "Seed: 2, Width: 0.2667, Coverage: 0.1143\n",
      "Seed: 3, Width: 0.2857, Coverage: 0.1238\n",
      "Seed: 4, Width: 0.2667, Coverage: 0.1048\n",
      "Seed: 5, Width: 0.2857, Coverage: 0.0952\n",
      "Seed: 6, Width: 0.2667, Coverage: 0.1048\n",
      "Seed: 7, Width: 0.2286, Coverage: 0.0667\n",
      "Seed: 8, Width: 0.2762, Coverage: 0.1143\n",
      "Seed: 9, Width: 0.2381, Coverage: 0.0762\n",
      "Seed: 10, Width: 0.2667, Coverage: 0.1143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running experiments:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 1, Width: 0.6974, Coverage: 0.0526\n",
      "Seed: 2, Width: 0.5921, Coverage: 0.0526\n",
      "Seed: 3, Width: 0.6447, Coverage: 0.0263\n",
      "Seed: 4, Width: 0.7632, Coverage: 0.0658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huanx\\AppData\\Local\\Temp\\ipykernel_49044\\1309679868.py:20: RuntimeWarning: Mean of empty slice.\n",
      "  covered[val_labels == j].mean() for j in range(np.unique(val_labels).max() + 1)\n",
      "C:\\Users\\huanx\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Running experiments: 100%|██████████| 10/10 [00:00<00:00, 121.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 5, Width: 0.6184, Coverage: 0.0395\n",
      "Seed: 6, Width: 0.6711, Coverage: 0.0789\n",
      "Seed: 7, Width: 0.6447, Coverage: 0.0658\n",
      "Seed: 8, Width: 0.6974, Coverage: 0.0658\n",
      "Seed: 9, Width: 0.6579, Coverage: 0.0526\n",
      "Seed: 10, Width: 0.6447, Coverage: 0.0658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running experiments:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 1, Width: 0.4300, Coverage: 0.1900\n",
      "Seed: 2, Width: 0.4000, Coverage: 0.2000\n",
      "Seed: 3, Width: 0.4200, Coverage: 0.1400\n",
      "Seed: 4, Width: 0.4700, Coverage: 0.1800\n",
      "Seed: 5, Width: 0.3900, Coverage: 0.1700\n",
      "Seed: 6, Width: 0.3500, Coverage: 0.1600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huanx\\AppData\\Local\\Temp\\ipykernel_49044\\1309679868.py:20: RuntimeWarning: Mean of empty slice.\n",
      "  covered[val_labels == j].mean() for j in range(np.unique(val_labels).max() + 1)\n",
      "C:\\Users\\huanx\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Running experiments: 100%|██████████| 10/10 [00:00<00:00, 133.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 7, Width: 0.5100, Coverage: 0.2200\n",
      "Seed: 8, Width: 0.4400, Coverage: 0.2100\n",
      "Seed: 9, Width: 0.5000, Coverage: 0.2000\n",
      "Seed: 10, Width: 0.4900, Coverage: 0.1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "folder_path = f'../model_logits/qwen/'\n",
    "for dimension in [\"cosmos\", \"drop\", \"esnli\", \"gsm8k\"]:\n",
    "        file_path = os.path.join(folder_path, f\"SocREval_{dimension}_logits.csv\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        X = df.iloc[:, :-1]\n",
    "        y = df.iloc[:, -1]\n",
    "        mean_time, std_time, mean_memory, std_memory =  calculate_statistics(X, y, num_runs=10, seed_start=1, dimension=dimension, dataset='SocREval')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.007449290005024522,\n",
       " 0.0005216576775678698,\n",
       " 0.19980039596557617,\n",
       " 0.0013422520293023142)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_time, std_time, mean_memory/1024/1024, std_memory/1024/1024"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
