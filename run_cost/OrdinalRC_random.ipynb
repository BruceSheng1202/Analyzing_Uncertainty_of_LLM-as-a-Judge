{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_number = {\n",
    "    'one': 1,\n",
    "    'two': 2,\n",
    "    'three': 3,\n",
    "    'four': 4,\n",
    "    'five': 5\n",
    "}\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "\n",
    "alpha = 0.10\n",
    "\n",
    "import json\n",
    "import math\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import os, sys\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import numpy as np\n",
    "\n",
    "# Base class for Ordinal Regression Ordinal Risk Predictor;\n",
    "# a typical calling order is: find_lambda -> calc_loss -> get_prediction_set_bounds;\n",
    "class OrdinalRegCRPredictor:\n",
    "\n",
    "    # calculate the bound [l, u] for the optimal prediction set\n",
    "    # such that sum of sy between the bound is greater than the lambda value.\n",
    "    @abstractmethod\n",
    "    def get_prediction_set_bounds(self, fyx, lambda_val):\n",
    "        pass\n",
    "\n",
    "    # calculate the incurred loss for a specific record\n",
    "    @abstractmethod\n",
    "    def calc_loss(self, fyx, y, lambda_val):\n",
    "        pass\n",
    "\n",
    "    # find the optimal value of lambda for a dataset\n",
    "    # such that the risk on this dataset is controlled by alpha\n",
    "    @abstractmethod\n",
    "    def find_lambda(self, fyxs, ys, alpha):\n",
    "        pass\n",
    "\n",
    "    # run prediction for a batch of records\n",
    "    @abstractmethod\n",
    "    def run_predictions(self, fyxs, ys, lambda_val):\n",
    "        pass\n",
    "\n",
    "class WeightedCRPredictor(OrdinalRegCRPredictor):\n",
    "\n",
    "    # initialize the weights\n",
    "    # normalize the weights so that the maximal value is 1\n",
    "    def __init__(self, hy):\n",
    "        max_hy = np.max(hy)\n",
    "        self.hy = hy / max_hy\n",
    "        self.num_classes = hy.size\n",
    "\n",
    "    # get the prediction set for given fyx and lambda_val\n",
    "    # this implementation is greedy\n",
    "    # it starts at the index with the largest sy=hy*fyx value\n",
    "    # then gradually extends the boundary until the risk meets the required limit\n",
    "    # after that, it tries to shrinks the boundary to squeeze the ones with zero risks\n",
    "    # the last step is needed to avoid producing a too large prediction set\n",
    "    def get_prediction_set_bounds(self, fyx, lambda_val):\n",
    "        sy = self.hy * fyx\n",
    "\n",
    "        #b_val = sum(sy) ## conditional\n",
    "        b_val = 1 ## marginal\n",
    "        threshold = b_val - lambda_val\n",
    "\n",
    "        index_max = np.argmax(sy)\n",
    "        s = sy[index_max]\n",
    "        l, u = index_max, index_max\n",
    "        while s < threshold:\n",
    "            if l - 1 >= 0 and u + 1 <= self.num_classes -1:\n",
    "                if sy[l - 1] >= sy[u + 1]:\n",
    "                    l = l - 1\n",
    "                    s = s + sy[l]\n",
    "                else:\n",
    "                    u = u + 1\n",
    "                    s = s + sy[u]\n",
    "            elif l - 1 >= 0:\n",
    "                l = l - 1\n",
    "                s = s + sy[l]\n",
    "            elif u + 1 <= self.num_classes - 1:\n",
    "                u = u + 1\n",
    "                s = s + sy[u]\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        while sy[l] == 0 and l < u:\n",
    "            l = l + 1\n",
    "        while sy[u] == 0 and l < u:\n",
    "            u = u - 1\n",
    "\n",
    "        return l, u\n",
    "\n",
    "    # calculate the incurred loss for a specific record\n",
    "    # hy:         weights of different labels;\n",
    "    # fyx:        model scores of different labels;\n",
    "    # y:          true label;\n",
    "    # lambda_val: a proposed risk bound;\n",
    "    def calc_loss(self, fyx, y, lambda_val):\n",
    "        lower_bound, upper_bound = self.get_prediction_set_bounds(fyx, lambda_val)\n",
    "        if (y >= lower_bound) and (y <= upper_bound):\n",
    "            return 0.0\n",
    "        else:\n",
    "            return self.hy[int(y)]\n",
    "\n",
    "    # find the optimal value of lambda such that the risk is controlled by alpha\n",
    "    # fyxs:  the matrix of model scores, where each row is for one record, each column is for one class;\n",
    "    # ys:    the array of true labels;\n",
    "    # alpha: risk bound, value between 0 and 1;\n",
    "    def find_lambda(self, fyxs, ys, alpha):\n",
    "        (num_records, num_classes) = fyxs.shape\n",
    "        b_val = 1 #max(hy)\n",
    "        threshold = (num_records + 1.0) / num_records * alpha - b_val / num_records\n",
    "\n",
    "        cur_lambda = 0.5\n",
    "        delta = 0.5\n",
    "        delta_threshold = 0.0005\n",
    "        while delta > delta_threshold:\n",
    "            total_r = 0.0\n",
    "            for i in range(num_records):\n",
    "                total_r = total_r + self.calc_loss(fyxs[i, :], ys[i], cur_lambda)\n",
    "            avg_r = total_r / num_records\n",
    "            if avg_r > threshold:\n",
    "                cur_lambda = cur_lambda - delta / 2\n",
    "            elif avg_r < threshold:\n",
    "                cur_lambda = cur_lambda + delta / 2\n",
    "            else:\n",
    "                break\n",
    "            delta = delta / 2\n",
    "        return cur_lambda\n",
    "\n",
    "    def run_predictions(self, fyxs, ys, lambda_val):\n",
    "        num_records = fyxs.shape[0]\n",
    "        prediction_sets = []\n",
    "        losses = []\n",
    "        for i in range(num_records):\n",
    "            lower_bound, upper_bound = self.get_prediction_set_bounds(fyxs[i, :], lambda_val)\n",
    "            prediction_sets.append((lower_bound, upper_bound))\n",
    "            label = int(ys[i])\n",
    "            if (label >= lower_bound) and (label <= upper_bound):\n",
    "                losses.append(0.0)\n",
    "            else:\n",
    "                losses.append(self.hy[label])\n",
    "        return prediction_sets, losses\n",
    "\n",
    "class DivergenceCRPredictor(OrdinalRegCRPredictor):\n",
    "\n",
    "    # Given fyx, calculate the cumulative head scores\n",
    "    # head_j = sum_{i=0}^{j}(fyx_i) / (K-1)\n",
    "    def get_head_scores(self, fyx):\n",
    "        num_classes = fyx.size\n",
    "        head_scores = np.zeros(num_classes)\n",
    "        prev = 0\n",
    "        for i in range(num_classes):\n",
    "            head_scores[i] = prev + fyx[i]\n",
    "            prev = head_scores[i]\n",
    "        return [v / (num_classes - 1) for v in head_scores]\n",
    "\n",
    "\n",
    "    # Given fyx, calculate the cumulative tail scores\n",
    "    # tail_j = sum_{i=j}^{K-1}(fyx_i) / (K-1)\n",
    "    def get_tail_scores(self, fyx):\n",
    "        num_classes = fyx.size\n",
    "        tail_scores = np.zeros(num_classes)\n",
    "        prev = 0\n",
    "        for i in range(num_classes - 1, -1, -1):\n",
    "            tail_scores[i] = prev + fyx[i]\n",
    "            prev = tail_scores[i]\n",
    "        return [v / (num_classes - 1) for v in tail_scores]\n",
    "\n",
    "    # Given fyx, as well as a threshold,\n",
    "    # find out the optimal bound [y_l, y_u] of the prediction set,\n",
    "    # such that the total risk is less than the threshold.\n",
    "    # this implementation is greedy.\n",
    "    def get_prediction_set_bounds(self, fyx, lambda_val):\n",
    "        num_classes = fyx.size\n",
    "        head_scores = self.get_head_scores(fyx)\n",
    "        tail_scores = self.get_tail_scores(fyx)\n",
    "\n",
    "        l, u = np.argmax(fyx), np.argmax(fyx)\n",
    "        sums = np.zeros(num_classes)\n",
    "        steps = [None] * num_classes\n",
    "        steps[0] = (l, u)\n",
    "        s = 0\n",
    "        for i in range(num_classes - 1):\n",
    "            if l > 0 and u < num_classes - 1:\n",
    "                if head_scores[l - 1] >= tail_scores[u + 1]:\n",
    "                    s = s + head_scores[l - 1]\n",
    "                    l = l - 1\n",
    "                else:\n",
    "                    s = s + tail_scores[u + 1]\n",
    "                    u = u + 1\n",
    "            elif l == 0:\n",
    "                s = s + tail_scores[u + 1]\n",
    "                u = u + 1\n",
    "            else:\n",
    "                s = s + head_scores[l - 1]\n",
    "                l = l - 1\n",
    "            sums[i + 1] = s\n",
    "            steps[i + 1] = (l, u)\n",
    "\n",
    "        for i in range(num_classes):\n",
    "            if sums[num_classes - 1] - sums[i] <= lambda_val:\n",
    "                l, u = steps[i][0], steps[i][1]\n",
    "                break\n",
    "\n",
    "        return l, u\n",
    "\n",
    "    # calculate the incurred loss for a specific record\n",
    "    # fyx:        model scores of different labels;\n",
    "    # y:          true label;\n",
    "    # lambda_val: a proposed risk bound;\n",
    "    def calc_loss(self, fyx, y, lambda_val):\n",
    "        num_classes = fyx.size\n",
    "        lower_bound, upper_bound = self.get_prediction_set_bounds(fyx, lambda_val)\n",
    "        if y < lower_bound:\n",
    "            return (lower_bound - y) / (num_classes - 1)\n",
    "        elif y > upper_bound:\n",
    "            return (y - upper_bound) / (num_classes - 1)\n",
    "        else:\n",
    "            return 0.0\n",
    "\n",
    "    # find the optimal value of lambda such that the risk is controlled by alpha\n",
    "    # fyxs:  the matrix of model scores, where each row is for one record, each column is for one class;\n",
    "    # ys:    the array of true labels;\n",
    "    # alpha: risk bound, value between 0 and 1;\n",
    "    def find_lambda(self, fyxs, ys, alpha):\n",
    "        (num_records, num_classes) = fyxs.shape\n",
    "        b_val = 1\n",
    "        threshold = (num_records + 1.0) / num_records * alpha - b_val / num_records\n",
    "\n",
    "        cur_lambda = 0.5\n",
    "        delta = 0.5\n",
    "        delta_threshold = 0.0001\n",
    "        while delta > delta_threshold:\n",
    "            total_r = 0.0\n",
    "            for i in range(num_records):\n",
    "                total_r = total_r + self.calc_loss(fyxs[i, :], ys[i], cur_lambda)\n",
    "            avg_r = total_r / num_records\n",
    "            if avg_r > threshold:\n",
    "                cur_lambda = cur_lambda - delta / 2\n",
    "            elif avg_r < threshold:\n",
    "                cur_lambda = cur_lambda + delta / 2\n",
    "            else:\n",
    "                break\n",
    "            delta = delta / 2\n",
    "        return cur_lambda\n",
    "\n",
    "    def run_predictions(self, fyxs, ys, lambda_val):\n",
    "        num_records = fyxs.shape[0]\n",
    "        num_classes = fyxs.shape[1]\n",
    "        prediction_sets = []\n",
    "        losses = []\n",
    "        for i in range(num_records):\n",
    "            lower_bound, upper_bound = self.get_prediction_set_bounds(fyxs[i, :], lambda_val)\n",
    "            prediction_sets.append((lower_bound, upper_bound))\n",
    "            label = int(ys[i])\n",
    "            if label < lower_bound:\n",
    "                losses.append((lower_bound - label) / (num_classes - 1))\n",
    "            elif label > upper_bound:\n",
    "                losses.append((label - upper_bound) / (num_classes - 1))\n",
    "            else:\n",
    "                losses.append(0.0)\n",
    "        return prediction_sets, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_data_load(filename, dimension):\n",
    "  with open(filename, 'r') as file:\n",
    "    content = file.read()\n",
    "  lines = content.splitlines()\n",
    "  data = []\n",
    "  for line in lines:\n",
    "    if line.strip():\n",
    "        try:\n",
    "            json_obj = json.loads(line)\n",
    "            data.append({'custom_id': json_obj['custom_id'], dimension: json_obj[dimension]})\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding line: {e}\")\n",
    "  y = pd.DataFrame(data)\n",
    "  y.set_index('custom_id', inplace=True)\n",
    "  y = y[dimension]\n",
    "  return y\n",
    "\n",
    "def softmax(logits):\n",
    "    exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "    return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "\n",
    "def run_experiment(X, y, seed, dataset, dimension):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    X.columns = list(range(len(X.columns)))\n",
    "    X = X.to_numpy().astype(np.float32)\n",
    "    y = y.to_numpy().astype(np.float32)-1\n",
    "\n",
    "    # labels = np.array([1, 4/3, 5/3, 2, 7/3, 8/3, 3, 10/3, 11/3, 4, 13/3, 14/3, 5])\n",
    "    # y = y*3\n",
    "    # from scipy.interpolate import interp1d\n",
    "    # n = X.shape[0]\n",
    "    # m = 13\n",
    "\n",
    "    # new_x = np.zeros((n, m))\n",
    "    # orig_idx = np.linspace(3, 15, 5)-3\n",
    "    # target_idx = np.linspace(3, 15, m)-3\n",
    "\n",
    "    # for i in range(n):\n",
    "    #     f = interp1d(orig_idx, X[i, :], kind='linear')\n",
    "    #     new_x[i, :] = f(target_idx)\n",
    "    \n",
    "    # X = softmax(new_x)   \n",
    "\n",
    "    # from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # fyxs_cal, fyxs_test, y_cal, y_test = train_test_split(X, y, test_size=0.5, random_state=seed)\n",
    "    # y_cal = y_cal.ravel()\n",
    "    # y_test = y_test.ravel()\n",
    "\n",
    "    # hy = np.ones(13)\n",
    "\n",
    "    # predictor = WeightedCRPredictor(hy)\n",
    "    # optimal_lambda = predictor.find_lambda(fyxs_cal, y_cal, alpha)\n",
    "    # prediction_sets, losses = predictor.run_predictions(fyxs_test, y_test, optimal_lambda)\n",
    "    # # predictor = DivergenceCRPredictor()\n",
    "    # # optimal_lambda = predictor.find_lambda(fyxs_cal, y_cal, alpha)\n",
    "    # # prediction_sets, losses = predictor.run_predictions(fyxs_test, y_test, optimal_lambda)\n",
    "\n",
    "    # y_qlow = np.array([interval[0] for interval in prediction_sets])/3+1\n",
    "    # y_qup = np.array([interval[1] for interval in prediction_sets])/3+1\n",
    "\n",
    "    # y_test_real = y_test/3+1\n",
    "\n",
    "    # labels = np.array([1, 2, 3, 4, 5])\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X = softmax(X)\n",
    "    fyxs_cal, fyxs_test, y_cal, y_test = train_test_split(X, y, test_size=0.5, random_state=seed)\n",
    "    y_cal = y_cal.ravel()\n",
    "    y_test = y_test.ravel()\n",
    "\n",
    "    hy = np.ones(5)\n",
    "\n",
    "    predictor = WeightedCRPredictor(hy)\n",
    "    optimal_lambda = predictor.find_lambda(fyxs_cal, y_cal, alpha)\n",
    "    prediction_sets, losses = predictor.run_predictions(fyxs_test, y_test, optimal_lambda)\n",
    "\n",
    "    # predictor = DivergenceCRPredictor()\n",
    "    # optimal_lambda = predictor.find_lambda(fyxs_cal, y_cal, alpha)\n",
    "    # prediction_sets, losses = predictor.run_predictions(fyxs_test, y_test, optimal_lambda)\n",
    "\n",
    "    y_qlow = np.array([interval[0] for interval in prediction_sets])+1\n",
    "    y_qup = np.array([interval[1] for interval in prediction_sets])+1\n",
    "\n",
    "    y_test_real = y_test+1\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'low':    y_qlow.ravel(),\n",
    "        'up':     y_qup.ravel(),\n",
    "        'y_test': y_test_real.ravel(),\n",
    "    })\n",
    "\n",
    "    df.to_csv(f'OrdinalRC_{dataset}_{dimension}_{seed}.csv', index=False)\n",
    "\n",
    "    intervals = [[(low, high)] for low, high in zip(y_qlow, y_qup)]\n",
    "\n",
    "    in_interval = [\n",
    "        any(low <= true_value <= high for low, high in sample_intervals)\n",
    "        for sample_intervals, true_value in zip(intervals, y_test_real)\n",
    "    ]\n",
    "\n",
    "    coverage_rate = np.mean(in_interval)\n",
    "    average_width = np.mean([high - low for sample_intervals in intervals for low, high in sample_intervals])  \n",
    "\n",
    "    print(f\"Seed: {seed}, Width: {average_width:.4f}, Coverage: {coverage_rate:.4f}\")\n",
    "\n",
    "    return average_width, coverage_rate\n",
    "\n",
    "import time\n",
    "import tracemalloc\n",
    "def calculate_statistics(X, y, num_runs=100, seed_start=1, dataset='Summeval', dimension='consistency'):\n",
    "    from tqdm import tqdm\n",
    "    timecost = []\n",
    "    memory = []\n",
    "    for i in tqdm(range(num_runs), desc=\"Running experiments\"):\n",
    "        seed = seed_start + i\n",
    "        tracemalloc.start()\n",
    "        start = time.perf_counter()\n",
    "        average_width, coverage_rate = run_experiment(X, y, seed, dataset, dimension)\n",
    "        end = time.perf_counter()\n",
    "        current, peak = tracemalloc.get_traced_memory()\n",
    "        tracemalloc.stop()\n",
    "        timecost.append(end - start)\n",
    "        memory.append(peak)\n",
    "\n",
    "    mean_time = np.mean(timecost)\n",
    "    std_time = np.std(timecost)\n",
    "    mean_memory = np.mean(memory)\n",
    "    std_memory = np.std(memory)\n",
    "\n",
    "    return  mean_time, std_time, mean_memory, std_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running experiments:  50%|█████     | 5/10 [00:01<00:01,  4.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 1, Width: 0.8163, Coverage: 0.6122\n",
      "Seed: 2, Width: 0.7653, Coverage: 0.5408\n",
      "Seed: 3, Width: 0.7449, Coverage: 0.5714\n",
      "Seed: 4, Width: 0.7143, Coverage: 0.6224\n",
      "Seed: 5, Width: 0.7449, Coverage: 0.5714\n",
      "Seed: 6, Width: 0.7449, Coverage: 0.5918\n",
      "Seed: 7, Width: 0.7755, Coverage: 0.5714\n",
      "Seed: 8, Width: 0.6735, Coverage: 0.5102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running experiments: 100%|██████████| 10/10 [00:01<00:00,  6.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 9, Width: 0.7755, Coverage: 0.5510\n",
      "Seed: 10, Width: 0.7449, Coverage: 0.5408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running experiments:  40%|████      | 4/10 [00:00<00:00, 39.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 1, Width: 0.3238, Coverage: 0.6000\n",
      "Seed: 2, Width: 0.3143, Coverage: 0.5619\n",
      "Seed: 3, Width: 0.3238, Coverage: 0.5429\n",
      "Seed: 4, Width: 0.3048, Coverage: 0.5905\n",
      "Seed: 5, Width: 0.3333, Coverage: 0.5429\n",
      "Seed: 6, Width: 0.3143, Coverage: 0.5714\n",
      "Seed: 7, Width: 0.2667, Coverage: 0.5333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running experiments:  80%|████████  | 8/10 [00:00<00:00, 38.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 8, Width: 0.3143, Coverage: 0.6095\n",
      "Seed: 9, Width: 0.2762, Coverage: 0.5714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running experiments: 100%|██████████| 10/10 [00:00<00:00, 38.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 10, Width: 0.3048, Coverage: 0.6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running experiments:  50%|█████     | 5/10 [00:00<00:00, 44.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 1, Width: 0.8553, Coverage: 0.8289\n",
      "Seed: 2, Width: 0.7500, Coverage: 0.7895\n",
      "Seed: 3, Width: 0.7895, Coverage: 0.7500\n",
      "Seed: 4, Width: 0.8816, Coverage: 0.8026\n",
      "Seed: 5, Width: 0.7763, Coverage: 0.7763\n",
      "Seed: 6, Width: 0.8421, Coverage: 0.8026\n",
      "Seed: 7, Width: 0.7763, Coverage: 0.7895\n",
      "Seed: 8, Width: 0.7895, Coverage: 0.8158\n",
      "Seed: 9, Width: 0.7500, Coverage: 0.7632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running experiments: 100%|██████████| 10/10 [00:00<00:00, 44.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 10, Width: 0.8026, Coverage: 0.7763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running experiments:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 1, Width: 0.4900, Coverage: 0.6700\n",
      "Seed: 2, Width: 0.4300, Coverage: 0.6300\n",
      "Seed: 3, Width: 0.4300, Coverage: 0.7000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running experiments:  40%|████      | 4/10 [00:00<00:00, 38.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 4, Width: 0.4900, Coverage: 0.7200\n",
      "Seed: 5, Width: 0.4200, Coverage: 0.7500\n",
      "Seed: 6, Width: 0.3900, Coverage: 0.7200\n",
      "Seed: 7, Width: 0.5300, Coverage: 0.7100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running experiments: 100%|██████████| 10/10 [00:00<00:00, 37.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 8, Width: 0.4500, Coverage: 0.7400\n",
      "Seed: 9, Width: 0.5400, Coverage: 0.7500\n",
      "Seed: 10, Width: 0.5000, Coverage: 0.7000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "folder_path = f'../model_logits/qwen/'\n",
    "for dimension in [\"cosmos\", \"drop\", \"esnli\", \"gsm8k\"]:\n",
    "        file_path = os.path.join(folder_path, f\"SocREval_{dimension}_logits.csv\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        X = df.iloc[:, :-1]\n",
    "        y = df.iloc[:, -1]\n",
    "        mean_time, std_time, mean_memory, std_memory =  calculate_statistics(X, y, num_runs=10, seed_start=1, dimension=dimension, dataset='SocREval')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.026801399991381914,\n",
       " 0.002336721704586695,\n",
       " 0.1935894012451172,\n",
       " 0.0010089274642808709)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_time, std_time, mean_memory/1024/1024, std_memory/1024/1024"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
